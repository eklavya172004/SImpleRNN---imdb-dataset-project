{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f98c594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TensorFlow and Keras installed!\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow keras --quiet\n",
    "print(\"✓ TensorFlow and Keras installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5341bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TensorFlow version: 2.16.1\n",
      "✓ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "print(f\"✓ TensorFlow version: {tf.__version__}\")\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a9f9a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Word index loaded! Vocabulary size: 88584\n",
      "Sample word mapping: [('fawn', 34701), ('tsukino', 52006), ('nunnery', 52007), ('sonja', 16816), ('vani', 63951)]\n"
     ]
    }
   ],
   "source": [
    "# Load IMDB word index and create reverse mapping\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "print(f\"✓ Word index loaded! Vocabulary size: {len(word_index)}\")\n",
    "print(f\"Sample word mapping: {list(word_index.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9220a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: g:\\simpleRNN\n",
      "Looking for model at: simple_rnn_imdb(1).h5\n",
      "Files in current directory:\n",
      "  - simple_rnn_imdb(1).h5\n",
      "✓ Model loaded from simple_rnn_imdb(1).h5\n",
      "✓ Model ready!\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,027</span> (5.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,313,027\u001b[0m (5.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,025</span> (5.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,025\u001b[0m (5.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Configuration\n",
    "model_path = 'simple_rnn_imdb(1).h5'\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "\n",
    "# Check current working directory and available files\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Looking for model at: {model_path}\")\n",
    "print(f\"Files in current directory:\")\n",
    "for f in os.listdir('.'):\n",
    "    if f.endswith('.h5'):\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "# Load the pre-trained model\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        model = load_model(model_path)\n",
    "        print(f\"✓ Model loaded from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error loading model: {str(e)[:100]}\")\n",
    "        # Build model from scratch if loading fails\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Embedding(max_features, 128, input_length=max_len),\n",
    "            keras.layers.SimpleRNN(128, activation='relu'),\n",
    "            keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        print(\"Using freshly initialized model\")\n",
    "else:\n",
    "    print(f\"⚠ Model file not found: {model_path}\")\n",
    "    print(\"Available .h5 files in current directory and subdirectories:\")\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        for f in files:\n",
    "            if f.endswith('.h5'):\n",
    "                print(f\"  - {os.path.join(root, f)}\")\n",
    "    # Build model from scratch\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Embedding(max_features, 128, input_length=max_len),\n",
    "        keras.layers.SimpleRNN(128, activation='relu'),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Using freshly initialized model\")\n",
    "\n",
    "print(\"✓ Model ready!\")\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad9a2aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions\n",
    "def decode_review(encoded_review):\n",
    "    \"\"\"Decode an encoded review back to words\"\"\"\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text using IMDB's word index.\n",
    "    Word indices 0-3 are reserved: 0=padding, 1=start, 2=OOV, 3=unused\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    encoded_review = []\n",
    "    for word in words:\n",
    "        # Check if word exists in dictionary\n",
    "        if word in word_index:\n",
    "            idx = word_index[word]\n",
    "            # Only use indices within training range\n",
    "            if idx < max_features:\n",
    "                # Add 3 to match Keras' imdb preprocessing (indices 0-3 are reserved)\n",
    "                encoded_review.append(idx + 3)\n",
    "            else:\n",
    "                # Word index out of range, use OOV token\n",
    "                encoded_review.append(2)\n",
    "        else:\n",
    "            # Word not found, use OOV token\n",
    "            encoded_review.append(2)\n",
    "    \n",
    "    # Pad to max_len\n",
    "    padded_review = sequence.pad_sequences([encoded_review], maxlen=max_len)\n",
    "    return padded_review\n",
    "\n",
    "def predict_review(review_text, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a given review text.\n",
    "    Returns: (sentiment, confidence_score, is_uncertain)\n",
    "    \"\"\"\n",
    "    preprocessed = preprocess_text(review_text)\n",
    "    prediction = model.predict(preprocessed, verbose=0)\n",
    "    confidence = float(prediction[0][0])\n",
    "    \n",
    "    # Determine sentiment with uncertainty detection\n",
    "    if abs(confidence - 0.5) < 0.1:  # Within 10% of threshold\n",
    "        sentiment = 'UNCERTAIN'\n",
    "        is_uncertain = True\n",
    "    elif confidence > confidence_threshold:\n",
    "        sentiment = 'Positive'\n",
    "        is_uncertain = False\n",
    "    else:\n",
    "        sentiment = 'Negative'\n",
    "        is_uncertain = False\n",
    "    \n",
    "    return sentiment, confidence, is_uncertain\n",
    "\n",
    "print(\"✓ Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "765d0741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE 1: Positive Review\n",
      "======================================================================\n",
      "Review: This movie was amazing! Great acting and brilliant plot. Highly recommended!\n",
      "Sentiment: Positive\n",
      "Confidence Score: 0.9856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Test with positive review\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE 1: Positive Review\")\n",
    "print(\"=\"*70)\n",
    "review_1 = \"This movie was amazing! Great acting and brilliant plot. Highly recommended!\"\n",
    "sentiment_1, score_1, uncertain_1 = predict_review(review_1)\n",
    "print(f\"Review: {review_1}\")\n",
    "print(f\"Sentiment: {sentiment_1}\")\n",
    "print(f\"Confidence Score: {score_1:.4f}\")\n",
    "if uncertain_1:\n",
    "    print(\"⚠ Model is UNCERTAIN about this prediction\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fda25da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE 2: Negative Review\n",
      "======================================================================\n",
      "Review: This movie was horrible! Bad acting and terrible plot. Waste of time.\n",
      "Sentiment: Negative\n",
      "Confidence Score: 0.2251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Test with negative review\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE 2: Negative Review\")\n",
    "print(\"=\"*70)\n",
    "review_2 = \"This movie was horrible! Bad acting and terrible plot. Waste of time.\"\n",
    "sentiment_2, score_2, uncertain_2 = predict_review(review_2)\n",
    "print(f\"Review: {review_2}\")\n",
    "print(f\"Sentiment: {sentiment_2}\")\n",
    "print(f\"Confidence Score: {score_2:.4f}\")\n",
    "if uncertain_2:\n",
    "    print(\"⚠ Model is UNCERTAIN about this prediction\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b268c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE 3: Batch Predictions\n",
      "======================================================================\n",
      "\n",
      "Testing 4 reviews:\n",
      "\n",
      "1. Review: Absolutely loved it! Best movie ever made.\n",
      "   Sentiment: Positive (Confidence: 0.9026)\n",
      "\n",
      "2. Review: Really disappointed with this film. Poor quality.\n",
      "   Sentiment: Positive (Confidence: 0.6402)\n",
      "\n",
      "3. Review: It was okay, nothing special.\n",
      "   Sentiment: Positive (Confidence: 0.6761)\n",
      "\n",
      "4. Review: Outstanding performance! A masterpiece!\n",
      "   Sentiment: Positive (Confidence: 0.8166)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Batch predictions on multiple reviews\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE 3: Batch Predictions\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "reviews = [\n",
    "    \"Absolutely loved it! Best movie ever made.\",\n",
    "    \"Really disappointed with this film. Poor quality.\",\n",
    "    \"It was okay, nothing special.\",\n",
    "    \"Outstanding performance! A masterpiece!\"\n",
    "]\n",
    "\n",
    "print(f\"\\nTesting {len(reviews)} reviews:\\n\")\n",
    "for i, review in enumerate(reviews, 1):\n",
    "    sentiment, score, uncertain = predict_review(review)\n",
    "    print(f\"{i}. Review: {review}\")\n",
    "    print(f\"   Sentiment: {sentiment} (Confidence: {score:.4f})\")\n",
    "    if uncertain:\n",
    "        print(f\"   ⚠ Model is UNCERTAIN about this prediction\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a8e88f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING ON REAL IMDB DATA\n",
      "======================================================================\n",
      "\n",
      "Model Accuracy on 100 IMDB test samples: 79.00%\n",
      "Model Loss: 0.4521\n",
      "\n",
      "======================================================================\n",
      "Sample Predictions on Real IMDB Reviews:\n",
      "======================================================================\n",
      "\n",
      "Review 1: ? please give this one a miss br br ? ? and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how mich...\n",
      "Actual: Negative\n",
      "Predicted: UNCERTAIN (Confidence: 0.5397)\n",
      "⚠ Model is UNCERTAIN - Confidence is too close to threshold\n",
      "\n",
      "Review 2: ? this film requires a lot of patience because it focuses on mood and character development the plot is very simple and many of the scenes take place ...\n",
      "Actual: Positive\n",
      "Predicted: Positive (Confidence: 0.9969)\n",
      "\n",
      "Review 3: ? many animation buffs consider ? ? the great forgotten genius of one special branch of the art puppet animation which he invented almost single ? and...\n",
      "Actual: Positive\n",
      "Predicted: Positive (Confidence: 0.9327)\n"
     ]
    }
   ],
   "source": [
    "# Load IMDB test data and test on real reviews\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING ON REAL IMDB DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load test data\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "X_test_padded = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "# Evaluate model on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_padded[:100], y_test[:100], verbose=0)\n",
    "print(f\"\\nModel Accuracy on 100 IMDB test samples: {test_accuracy:.2%}\")\n",
    "print(f\"Model Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Show predictions on actual IMDB reviews\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Sample Predictions on Real IMDB Reviews:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(3):\n",
    "    decoded = decode_review(X_test[i])\n",
    "    actual = \"Positive\" if y_test[i] == 1 else \"Negative\"\n",
    "    \n",
    "    # Get prediction with raw confidence\n",
    "    preprocessed = X_test_padded[i:i+1]\n",
    "    raw_pred = model.predict(preprocessed, verbose=0)[0][0]\n",
    "    predicted, score, uncertain = predict_review(decoded)\n",
    "    \n",
    "    print(f\"\\nReview {i+1}: {decoded[:150]}...\")\n",
    "    print(f\"Actual: {actual}\")\n",
    "    print(f\"Predicted: {predicted} (Confidence: {score:.4f})\")\n",
    "    if uncertain:\n",
    "        print(f\"⚠ Model is UNCERTAIN - Confidence is too close to threshold\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
